# -*- coding: utf-8 -*-
"""ProyectoV2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18wSwYH8nadhCj6ivLyCWxeYN_yc0xrik
"""

# Calculate skewness for each feature to understand their distributions
import numpy as np
import pandas as pd
import seaborn as sns
import dash
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, PowerTransformer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from scipy.stats import skew
from sklearn.neighbors import NearestNeighbors
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.express as px

app = dash.Dash(__name__)

# Load the dataset
dataset_path = "dataset.xlsx"
df = pd.read_excel(dataset_path)

df.head()

# Display basic information about the dataset, including column names and data types
df.info()

# Check for the quick overview of the main statistical metrics for numerical data
df.describe()

# Check for missing values (NaN) in the dataset
df.isna().sum()

# Convert 'grip_lost' and 'Robot_ProtectiveStop' to float
# This is done to ensure consistency by having same datatypes as well as for ease of using mathematical operations that required numerical values
df['grip_lost'] = df['grip_lost'].astype(float)
df['Robot_ProtectiveStop'] = df['Robot_ProtectiveStop'].astype(float)

# Calculate the proportion of missing values to decide if we need to drop them or replace with mean
missing_proportion = df.isnull().mean()
print(missing_proportion)

# Show the plot
# Visualize missing values using a heatmap
# cbar=False: This parameter controls whether or not to display the color bar (cbar) alongside the heatmap.
# The color bar is a scale that shows how the colors map to data values. Setting cbar=False hides this scale.
# cmap='viridis': This parameter specifies the color map to use for the heatmap.
# 'viridis' is a color map that transitions from dark blue to yellow, providing a good contrast to visualize the presence (or absence) of data.
# Pattern Identification: Helps to understand the distribution of null values across the columns
sns.heatmap(df.isnull(), cbar=True, cmap='viridis')
plt.show()

# Print the total number of rows in the dataset
# Remove rows with null values
# We decided to drop the rows since more than 20 columns were having null values for these rows.
df.dropna(inplace=True)
df.isnull().sum()
print(f'Checking length of the entire data frame {len(df)}')

# Drop the 'Timestamp' and 'Num' columns
df.drop(columns=['Timestamp', 'Num'], inplace=True)

df.corr()

# Print the total number of rows in the dataset
# Select columns for normalization (excluding 'grip_lost' and 'Robot_ProtectiveStop')
# removed these two columns since normalizing binary data does not make sense
columns_to_normalize = df.columns.difference(['grip_lost', 'Robot_ProtectiveStop'])
print(f'Checking length of the entire data frame {len(df)}')

# Show the plot
# Visualize outliers using box plots
plt.figure(figsize=(15, 10))
df[columns_to_normalize].boxplot(rot=45)
plt.title('Box plot to visualize outliers')
plt.show()

Q1 = df[columns_to_normalize].quantile(0.25)
Q3 = df[columns_to_normalize].quantile(0.75)
IQR = Q3 - Q1

# Display Q1, Q3, and IQR for verification
print("Q1:\n", Q1)
print("\nQ3:\n", Q3)
print("\nIQR:\n", IQR)

print(f'Checking length of the entire data frame {len(df)}')

print(f'Checking length of the entire data frame {len(df)}')

# Create a dictionary to store the counts
value_distribution = {}

# Check for positive, negative, and zero values
for column in columns_to_normalize:
    num_positive = (df[column] > 0).sum()
    num_negative = (df[column] < 0).sum()
    num_zero = (df[column] == 0).sum()

    # Store the counts in the dictionary
    value_distribution[column] = {
        'positive': num_positive,
        'zero': num_zero,
        'negative': num_negative
    }


value_distribution_df = pd.DataFrame(value_distribution).T


print(value_distribution_df)


for col in columns_to_normalize:
    lower_bound = Q1[col] - 1.5 * IQR[col]
    upper_bound = Q3[col] + 1.5 * IQR[col]
    df[col] = df[col].apply(lambda x: max(min(x, upper_bound), lower_bound))

print(f'Checking length of the entire data frame {len(df)}')

df[columns_to_normalize] += 1 - df[columns_to_normalize].min()

skewness = df[columns_to_normalize].apply(lambda x: skew(x))
print("Skewness of the features before transformation:")
print(skewness)

# Show the plot
# Visualize skewness using histograms
df[columns_to_normalize].hist(bins=30, figsize=(15, 10))
plt.suptitle('Histograms of features before transformation')
plt.show()

# Normalize the selected columns
# Normalization Adjusts the values in the dataset to a common scale without distorting differences in the ranges of values.
# It typically refers to scaling data to a range between 0 and 1.
# Scaling used When features have different units of measurement and need to be on the same scale.
# When applying algorithms that calculate distances between data points (e.g., clustering algorithms, K-Nearest Neighbors).

scaler = MinMaxScaler()
df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

# MinMaxScaler scales features to a given range (by default between 0 and 1)

print(f'Checking length of the entire data frame {len(df)}')

from sklearn.preprocessing import PowerTransformer


pt = PowerTransformer(method='yeo-johnson')

# Apply the transformation to the specified columns
df[columns_to_normalize] = pt.fit_transform(df[columns_to_normalize])

skewness_after = df[columns_to_normalize].apply(lambda x: skew(x))
print("Skewness of the features after transformation:")
print(skewness_after)

# Show the plot
# Visualize histograms after transformation
df[columns_to_normalize].hist(bins=30, figsize=(15, 10))
plt.suptitle('Histograms of features after transformation')
plt.show()

# Show the plot
# Plot the correlation matrix
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Print the first few rows of the dataframe
print(df.head())

# Optional: Dimensionality Reduction using PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(df)
print(f'Checking length of the entire data frame {len(df)}')

# pca = PCA(n_components=len(columns_to_normalize))
# data_pca = pca.fit_transform(df[columns_to_normalize])

# cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)
# plt.plot(range(1, len(columns_to_normalize) + 1), cumulative_var_ratio, marker='o', linestyle='--')
# plt.xlabel('Number of Components')
# plt.ylabel('Cumulative Explained Variance Ratio')
# plt.title('Cumulative Variance Explained by Principal Components')
# plt.grid(True)
# plt.show()

# Apply K-Means clustering algorithm
# K-Means Clustering
# Determine optimal number of clusters using the Elbow Method
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_pca)
    sse.append(kmeans.inertia_)

# Show the plot
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), sse, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('SSE')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.show()

# Apply K-Means clustering algorithm
# Choose the number of clusters (e.g., 3) based on the elbow plot
n_clusters = 2
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans.fit(data_pca)
df['KMeans_Cluster'] = kmeans.labels_

# Show the plot
# Visualize the K-Means clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=df['KMeans_Cluster'], palette='viridis')
plt.title('K-Means Clusters Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Apply K-Means clustering algorithm
# Print the silhouette score for K-Means
kmeans_silhouette_avg = silhouette_score(data_pca, df['KMeans_Cluster'])
print(f'K-Means Silhouette Score: {kmeans_silhouette_avg}')

# K-Means Silhouette Score: 0.6102758974479

# Show the plot
# Hierarchical Clustering
# Perform hierarchical clustering using Ward's method
linked = linkage(data_pca, method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 7))
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Perform hierarchical clustering and create a linkage matrix
# Hyperparameter tuning for Hierarchical Clustering
param_grid = {'n_clusters': list(range(2, 11)),
              'linkage': ['ward', 'complete', 'average', 'single']}


# Ward: Minimizes the variance of the clusters being merged.
# Complete: Uses the maximum distance between all observations of pairs of clusters.
# Average: Uses the average of the distances of each observation of pairs of clusters.
# Single: Uses the minimum distances between all observations of pairs of clusters.

# Perform grid search with silhouette score as the metric
# It is initialized to -1 as a starting point, assuming that silhouette scores are non-negative.
# Initialized to -1 as a starting point because silhouette scores range from -1 to 1,
# where 1 indicates dense, well-separated clusters,
# 0 indicates overlapping clusters, and -1 indicates incorrect clustering.
best_silhouette = -1
best_params = None
best_model = None

# Calculate the silhouette score to evaluate clustering performance
for linkage_method in param_grid['linkage']:
    for n_clusters in param_grid['n_clusters']:
        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage_method)
        labels = model.fit_predict(data_pca)
        silhouette_avg = silhouette_score(data_pca, labels)
        print(f'Linkage: {linkage_method}, Clusters: {n_clusters}, Silhouette Score: {silhouette_avg}')
        if silhouette_avg > best_silhouette:
            best_silhouette = silhouette_avg
            best_params = {'n_clusters': n_clusters, 'linkage': linkage_method}
            best_model = model

print(f'Best Silhouette Score for Hierarchical Clustering: {best_silhouette}')
print(f'Best Parameters for Hierarchical Clustering: {best_params}')

# Add the best cluster labels from Hierarchical Clustering to the original dataframe
df['Hierarchical_Cluster'] = best_model.labels_

# Show the plot
# Visualize the Hierarchical clusters
# [:, 0] represents the first PCA component
# [:, 1] represents the second PCA component.
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=df['Hierarchical_Cluster'], palette='viridis')
plt.title('Hierarchical Clusters Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Show the plot
# DBSCAN Clustering
# Find the optimal epsilon using the k-distance graph

# distances, indices = neighbors_fit.kneighbors(data_pca):
# Computes the distances and indices of the 5 nearest neighbors for each data point in data_pca.
# distances = np.sort(distances[:, 4], axis=0): Sorts and selects the distances to the 5th nearest neighbor (n_neighbors=5)
# for each data point. This array of distances will be used to plot the k-distance graph.
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(data_pca)
distances, indices = neighbors_fit.kneighbors(data_pca)
distances = np.sort(distances[:, 4], axis=0)


plt.figure(figsize=(8, 5))
plt.plot(distances)
plt.xlabel('Data Points sorted by distance')
plt.ylabel('Epsilon')
plt.title('K-Distance Graph for DBSCAN')
plt.show()

# From the plot, choose an appropriate epsilon (e.g., the point of maximum curvature)
epsilon = 0.05  # Adjust this value based on the K-Distance Graph

# Apply DBSCAN clustering algorithm
# Perform DBSCAN clustering
dbscan = DBSCAN(eps=epsilon, min_samples=5)
dbscan_labels = dbscan.fit_predict(data_pca)

# Add the DBSCAN cluster labels to the original dataframe
df['DBSCAN_Cluster'] = dbscan_labels

# Visualize the DBSCAN clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=df['DBSCAN_Cluster'], palette='viridis')
plt.title('DBSCAN Clusters Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Print the silhouette score for DBSCAN clustering (only if there is more than one cluster)
if len(set(dbscan_labels)) > 1:
    dbscan_silhouette_avg = silhouette_score(data_pca, df['DBSCAN_Cluster'])
    print(f'DBSCAN Silhouette Score: {dbscan_silhouette_avg}')

# Crear listas de columnas, verificando que existan en el DataFrame
temp_cols = [col for col in ['Temperature_T0', 'Temperature_J1', 'Temperature_J2', 'Temperature_J3', 'Temperature_J4', 'Temperature_J5'] if col in df.columns]
current_cols = [col for col in ['Current_J0', 'Current_J1', 'Current_J2', 'Current_J3', 'Current_J4', 'Current_J5'] if col in df.columns]


# Layout de la aplicación
app.layout = html.Div([
    html.H1('Dashboard de Comparación'),

    dcc.Dropdown(
        id='variable-dropdown',
        options=[
            {'label': 'Temperaturas', 'value': 'temp'},
            {'label': 'Corrientes', 'value': 'current'},
        ],
        value='temp'
    ),

    dcc.Graph(id='time-series-graph'),

    dcc.RangeSlider(
        id='index-slider',
        min=0,
        max=len(df) - 1,
        value=[0, len(df) - 1],
        marks={0: 'Inicio', len(df) - 1: 'Fin'},
        step=1
    )
])

# Callback para actualizar el gráfico
@app.callback(
    Output('time-series-graph', 'figure'),
    [Input('variable-dropdown', 'value'),
     Input('index-slider', 'value')]
)
def update_graph(selected_var, index_range):
    start_index, end_index = index_range
    filtered_df = df.iloc[start_index:end_index+1]

    if selected_var == 'temp':
        cols = temp_cols
        title = 'Comparación de Temperaturas'
    elif selected_var == 'current':
        cols = current_cols
        title = 'Comparación de Corrientes'


    # Reestructurar los datos
    melted_df = pd.melt(filtered_df.reset_index(), id_vars=['index'], value_vars=cols, var_name='Variable', value_name='Valor')

    fig = px.line(melted_df, x='index', y='Valor', color='Variable', title=title)
    fig.update_layout(xaxis_title='Índice', yaxis_title='Valor')

    return fig

if __name__ == '__main__':
    app.run_server(debug=True, port=8060)